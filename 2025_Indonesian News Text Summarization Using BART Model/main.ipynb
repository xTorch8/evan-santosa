{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affc2305",
   "metadata": {},
   "source": [
    "# Indonesian News Text Summarization Using BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada3846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "import torch \n",
    "import transformers\n",
    "import datasets\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3bb52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe1fbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f292a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 512\n",
    "max_target = 128\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce145c",
   "metadata": {},
   "source": [
    "### I. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e5854",
   "metadata": {},
   "source": [
    "The dataset used for this project is the IndoSum Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b87308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3385d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_paths):\n",
    "    articles = []\n",
    "    summaries = []\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                article_text = \" \".join([\" \".join(sentence) for paragraph in data[\"paragraphs\"] for sentence in paragraph])\n",
    "                summary_text = \" \".join([\" \".join(sentence) for sentence in data[\"summary\"]])\n",
    "\n",
    "                articles.append(article_text)\n",
    "                summaries.append(summary_text)\n",
    "    return articles, summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a69aa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 400\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 50\n",
      "})\n",
      "Dataset({\n",
      "    features: ['document', 'summary'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../data/indosum/indosum\"\n",
    "\n",
    "train_files = sorted(glob.glob(os.path.join(base_path, \"train.0[1-5].jsonl\")))\n",
    "dev_files = sorted(glob.glob(os.path.join(base_path, \"dev.0[1-5].jsonl\")))\n",
    "test_files = sorted(glob.glob(os.path.join(base_path, \"test.0[1-5].jsonl\")))\n",
    "\n",
    "train_articles, train_summaries = load_data(train_files)\n",
    "dev_articles, dev_summaries = load_data(dev_files)\n",
    "test_articles, test_summaries = load_data(test_files)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"document\": train_articles[:400], \"summary\": train_summaries[:400]})\n",
    "val_dataset = Dataset.from_dict({\"document\": dev_articles[400:450], \"summary\": dev_summaries[400:450]})\n",
    "test_dataset = Dataset.from_dict({\"document\": test_articles[450:500], \"summary\": test_summaries[450:500]})\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec130dd6",
   "metadata": {},
   "source": [
    "### II. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d8717",
   "metadata": {},
   "source": [
    "The preprocessing technique used for this project is the BART tokenizer. The BART tokenizer is a subword tokenizer used with the BART (Bidirectional and Auto-Regressive Transformer) model. It is based on Byte-Pair Encoding (BPE) and uses SentencePiece to handle tokenization. The tokenizer work as follows:\n",
    "- Step 1: Preprocessing\n",
    "    - The input text is lowercased and normalized (handles Unicode characters, punctuation, and spacing).\n",
    "    - It can process unseen words using subword tokenization.\n",
    "- Step 2: Tokenization (Subword Splitting)\n",
    "    - The tokenizer breaks words into subwords using Byte-Pair Encoding (BPE).\n",
    "    - Common words remain whole (\"hello\" → [\"hello\"]), while rare words split into subwords (\"unhappiness\" → [\"un\", \"happiness\"]).\n",
    "- Step 3: Convert Tokens to IDs\n",
    "    - Each token (or subword) is mapped to a unique integer ID from the vocabulary.\n",
    "    - Example:\n",
    "        - \"Hello World\"\n",
    "        - tensor([[    0,  31414,   232,     2]])\n",
    "- Step 4: Special Tokens\n",
    "    - BART uses special tokens for sequence modeling:\n",
    "        - ```<s>``` (Start of sentence)\n",
    "        - ```</s>``` (End of sentence)\n",
    "        - ```<mask>``` (Masked token for denoising pretraining)\n",
    "        - ```<pad>``` (Padding token for batching)\n",
    "- Step 5: Decoding (Reverse Tokenization)\n",
    "    - The model generates output as token IDs, which the tokenizer converts back to human-readable text.\n",
    "    - Example:\n",
    "        - tensor([[    0,  31414,   232,     2]])\n",
    "        - \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eb56493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68383f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a053d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_to_process):\n",
    "  inputs = [dialogue for dialogue in data_to_process[\"document\"]]\n",
    "\n",
    "  model_inputs = tokenizer(inputs,  max_length = max_input, padding = \"max_length\", truncation = True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    targets = tokenizer(data_to_process[\"summary\"], max_length = max_target, padding = \"max_length\", truncation = True)\n",
    "    \n",
    "  model_inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "\n",
    "  return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7bddebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/400 [00:00<?, ? examples/s]c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 400/400 [00:00<00:00, 2719.10 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 3008.48 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1806.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_data, batched = True)\n",
    "val_dataset = val_dataset.map(preprocess_data, batched = True)\n",
    "test_dataset = test_dataset.map(preprocess_data, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a81982f",
   "metadata": {},
   "source": [
    "### III. Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37237aa1",
   "metadata": {},
   "source": [
    "The model used for this project is the BART model. BART is a transformer model introduced by Facebook AI, that combines bidirectional and autoregressive transformers. BART uses encoder-decoder architecture that is essential for tasks involving sequences of events, such as summarization. The bidirectional approach allows the model to capture contextual information, understanding, and representing input text from both directions. Meanwhile, the autoregressive approach allows the model to create coherent and contextually rich abstractive summaries.\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_9.49.47_PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e53f0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b35c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dec644af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "    result = metric.compute(predictions = decoded_preds, references = decoded_labels, use_stemmer = True)\n",
    "\n",
    "    result = {key: value * 100 for key, value in result.items()}  \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9435baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6e0cb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    \"../models/bart-v2\", \n",
    "    evaluation_strategy = \"steps\",\n",
    "    save_steps = 100,\n",
    "    eval_steps = 100,    \n",
    "    logging_steps = 10,\n",
    "    warmup_steps = 500,    \n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1,\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 2,\n",
    "    num_train_epochs = 3,\n",
    "    predict_with_generate = True,\n",
    "    eval_accumulation_steps = 1,\n",
    "    fp16 = True   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "448b868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20c062cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_14292\\386426612.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = val_dataset,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "324bafb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 36:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.253800</td>\n",
       "      <td>0.593706</td>\n",
       "      <td>69.506260</td>\n",
       "      <td>62.686629</td>\n",
       "      <td>67.468269</td>\n",
       "      <td>67.220292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.539372</td>\n",
       "      <td>69.414499</td>\n",
       "      <td>63.093253</td>\n",
       "      <td>67.636876</td>\n",
       "      <td>67.479265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.562498</td>\n",
       "      <td>67.590426</td>\n",
       "      <td>61.283295</td>\n",
       "      <td>65.789854</td>\n",
       "      <td>65.563980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.527855</td>\n",
       "      <td>68.851669</td>\n",
       "      <td>62.566604</td>\n",
       "      <td>67.281835</td>\n",
       "      <td>66.957766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.553991</td>\n",
       "      <td>69.488487</td>\n",
       "      <td>61.867581</td>\n",
       "      <td>67.478255</td>\n",
       "      <td>67.277570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.526198</td>\n",
       "      <td>72.296630</td>\n",
       "      <td>64.842544</td>\n",
       "      <td>70.355759</td>\n",
       "      <td>70.082403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.497700</td>\n",
       "      <td>0.515091</td>\n",
       "      <td>72.490772</td>\n",
       "      <td>65.555322</td>\n",
       "      <td>70.832354</td>\n",
       "      <td>70.595559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.585800</td>\n",
       "      <td>0.528015</td>\n",
       "      <td>72.856121</td>\n",
       "      <td>65.351684</td>\n",
       "      <td>70.887255</td>\n",
       "      <td>70.695226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.515244</td>\n",
       "      <td>73.660304</td>\n",
       "      <td>65.727714</td>\n",
       "      <td>71.743835</td>\n",
       "      <td>71.512206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.518887</td>\n",
       "      <td>73.477913</td>\n",
       "      <td>65.942303</td>\n",
       "      <td>71.620595</td>\n",
       "      <td>71.370887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>71.997558</td>\n",
       "      <td>64.358914</td>\n",
       "      <td>69.362354</td>\n",
       "      <td>69.128263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.488533</td>\n",
       "      <td>72.414225</td>\n",
       "      <td>64.725228</td>\n",
       "      <td>70.273767</td>\n",
       "      <td>70.075340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\main-gpu\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=0.45249420702457427, metrics={'train_runtime': 2220.2933, 'train_samples_per_second': 0.54, 'train_steps_per_second': 0.54, 'total_flos': 1300262761267200.0, 'train_loss': 0.45249420702457427, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "817c2f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_results = trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c4f4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39869368076324463, 'eval_rouge1': 71.62222408857505, 'eval_rouge2': 64.4430412437017, 'eval_rougeL': 69.19834755752343, 'eval_rougeLsum': 68.7715976250652, 'eval_runtime': 108.6679, 'eval_samples_per_second': 0.46, 'eval_steps_per_second': 0.46, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc8ea59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bart-v2\\\\tokenizer_config.json',\n",
       " '../models/bart-v2\\\\special_tokens_map.json',\n",
       " '../models/bart-v2\\\\vocab.json',\n",
       " '../models/bart-v2\\\\merges.txt',\n",
       " '../models/bart-v2\\\\added_tokens.json',\n",
       " '../models/bart-v2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"../models/bart-v2\")\n",
    "tokenizer.save_pretrained(\"../models/bart-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0abee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merdeka.com - Indonesia Corruption Watch ( ICW ) meminta Komisi Pemberantas   Korupsi ( KPK ) ikut memantau perkembangan atas meninggalnya saksi kunci kasus mega korupsi e - KTP , Johannes Marliem . Peneliti ICW Divisi Hukum dan Monitoring Peradila , Aradila Caesar mengatakan momentum meninggalnya saksi kunci tersebut menimbulkan kejanggalan dan tanda tanya besar . \" Orang meninggal kita kan tidak bisa prediksi itu bukan kuasa kita . Tapi kalau kita melihat momentum kan ada suatu kejanggalan . Kenapa momentum meninggalnya , saat kasus e - ktp sedang ditangani oleh KPK , \" katanya seusai konferensi pers di Kantor Sekeretariatan ICW ,   Jakarta , Minggu(13 / 8 ) . Pihak ICW meminta KPK turut menyelidiki kematian saksi kunci ini dan menjelaskan kepada masyarakat apakah ada keterkaitan dengan permasalahan korupsi e - KTP atau hal-hal lain dibalik kematian Johannes . \" Kita minta KPK dan juga bekerja sama dengan pihak otoritas untuk menyelidiki kematian dari saksi kunci tersebut dengan serius . Artinya nanti KPK harus bisa menjelaskan kepada publik kenapa kematiannya , \" pintanya . ICW berharap dengan kematian saksi kunci , tidak membuat efek negatif untuk permasalahan kasus e - KTP ini . \" Jangan sampai kematiannya berdampak negatif dalam konteks membongkar kasus e - ktp tersebut , \" pungkasnya . [ ded ]\n",
      "Indonesia Corruption Watch ( ICW ) meminta Komisi Pemberantas Korupsi   ( KPK ) ikut memantau perkembangan atas meninggalnya saksi kunci kasus mega   korupsi   e - KTP , Johannes Marliem dan menjelaskan kepada masyarakat apakah ada keterkaitan dengan permasalahan korupsi e - KTP atau hal-hal lain dibalik kematian Johannes . Peneliti ICW Divisi Hukum dan Monitoring Peradila , Aradila Caesar mengatakan momentum meninggalnya saksi kunci tersebut menimbulkan kejanggalan dan tanda tanya besar .\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[4]\n",
    "\n",
    "print(sample[\"document\"])\n",
    "\n",
    "print(sample[\"summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
