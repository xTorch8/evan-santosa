{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1df778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "from pycocotools.coco import COCO\n",
    "from coco_eval import CocoEvaluator\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be241c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy paths - replace these\n",
    "BASE_DIR = \"./Road-Damage-Indonesia-4/test\"\n",
    "ANNOTATION_FILE = \"./Road-Damage-Indonesia-4/test/_annotations.coco.json\"\n",
    "IMAGE_DIR = BASE_DIR\n",
    "\n",
    "# Replace with your Roboflow API info\n",
    "API_KEY = \"pt6GF9eJQFZMOFNFL7sc\"\n",
    "MODEL_ID = \"road-damage-indonesia-fou2m/1\"\n",
    "\n",
    "CLIENT = InferenceHTTPClient(\n",
    "    api_url = \"https://detect.roboflow.com\",\n",
    "    api_key = API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae466f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./Road-Damage-Indonesia-4/test\\\\X_27330_jpeg.rf.b1af74b216d0542841dd23ca817fdc71.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_27340_jpeg.rf.4af9e108a84c2761e5fb60b44f0bbdf2.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_9320_jpeg.rf.bae4afd615668c06c2c174cc14d8de6f.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_2810_jpeg.rf.3e8d39d3e615feef9df9ed3f3da06473.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_17270_jpeg.rf.10b9aae5082f769ca285bea4e1985f92.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_31920_jpeg.rf.6de7e47078672dac0d47dc8a8e4e8c48.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_6210_jpeg.rf.75e90db30e3482f51d92509585eb2035.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_27220_jpeg.rf.fac59348b40dc5052e7931ef5b9c7272.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_31940_jpeg.rf.d467a1dec0c41c989650f91336e343c7.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_29650_jpeg.rf.a3f36abccfaa32ba8b6d870c5c372349.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_32730_jpeg.rf.9ab472b0c212dfbf8f29d8b7dc947034.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_34920_jpeg.rf.bb95944afeb194d686a227f3837cda16.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_30660_jpeg.rf.43d3a2d6f3090da9e3a36c150b90cb2e.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_32650_jpeg.rf.7f2dc343c88f511068bdd1980fc2c118.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_30000_jpeg.rf.333a86f0061c468a60d7ad2ef23863db.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_32770_jpeg.rf.9a86fc05f2d1d4b150bd0885d37f8c70.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_36750_jpeg.rf.c8013362f341f77fcb49075d21ee234d.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_33240_jpeg.rf.0a09091beff9015d5a263912d860f46a.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_36830_jpeg.rf.751052b6f304dcbe4572786a3f07a3dd.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_33110_jpeg.rf.d9ff203e3bf5a443392c191d54eb79ca.jpg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_gt = COCO(ANNOTATION_FILE)\n",
    "image_ids = coco_gt.getImgIds()\n",
    "image_paths = [os.path.join(IMAGE_DIR, coco_gt.loadImgs(i)[0]['file_name']) for i in image_ids[:10]]\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af37595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build class name to ID map\n",
    "categories = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "CLASS_NAME_TO_ID = {cat['name']: cat['id'] for cat in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4465157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.851\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.491\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.700\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.384\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.573\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.368\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.523\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.523\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.700\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.589\n",
      "Precision @ IoU=0.50: 0.708\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageEnhance\n",
    "from torchvision.ops import nms\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. TTA AUGMENTATION ---\n",
    "\n",
    "def get_augmented_images(img):\n",
    "    img = img.convert(\"RGB\")\n",
    "    img_tensor = TF.to_tensor(img)\n",
    "    augmentations = []\n",
    "\n",
    "    augmentations.append((\"original\", img_tensor))\n",
    "\n",
    "    # Flip horizontal\n",
    "    flipped = torch.flip(img_tensor, dims=[2])\n",
    "    augmentations.append((\"hflip\", flipped))\n",
    "\n",
    "    # Resize smaller and back\n",
    "    small = TF.resize(img_tensor, [int(img_tensor.shape[1] * 0.75), int(img_tensor.shape[2] * 0.75)])\n",
    "    small = TF.resize(small, [img_tensor.shape[1], img_tensor.shape[2]])\n",
    "    augmentations.append((\"scale_down\", small))\n",
    "\n",
    "    # Resize larger and back\n",
    "    large = TF.resize(img_tensor, [int(img_tensor.shape[1] * 1.25), int(img_tensor.shape[2] * 1.25)])\n",
    "    large = TF.resize(large, [img_tensor.shape[1], img_tensor.shape[2]])\n",
    "    augmentations.append((\"scale_up\", large))\n",
    "\n",
    "    # Brightness\n",
    "    bright_img = ImageEnhance.Brightness(TF.to_pil_image(img_tensor)).enhance(1.5)\n",
    "    bright = TF.to_tensor(bright_img)\n",
    "    augmentations.append((\"bright\", bright))\n",
    "\n",
    "    # Contrast\n",
    "    contrast_img = ImageEnhance.Contrast(TF.to_pil_image(img_tensor)).enhance(1.3)\n",
    "    contrast = TF.to_tensor(contrast_img)\n",
    "    augmentations.append((\"contrast\", contrast))\n",
    "\n",
    "    return augmentations\n",
    "\n",
    "\n",
    "# --- 2. FLIP CORRECTION ---\n",
    "\n",
    "def undo_horizontal_flip(preds, width):\n",
    "    for pred in preds.get(\"predictions\", []):\n",
    "        pred[\"x\"] = width - pred[\"x\"]\n",
    "    return preds\n",
    "\n",
    "\n",
    "# --- 3. AGGREGATE PREDICTIONS ---\n",
    "\n",
    "def aggregate_predictions(preds_list, iou_thresh=0.5):\n",
    "    all_boxes, all_scores, all_labels = [], [], []\n",
    "\n",
    "    for preds in preds_list:\n",
    "        for pred in preds.get(\"predictions\", []):\n",
    "            x1 = pred[\"x\"] - pred[\"width\"] / 2\n",
    "            y1 = pred[\"y\"] - pred[\"height\"] / 2\n",
    "            x2 = pred[\"x\"] + pred[\"width\"] / 2\n",
    "            y2 = pred[\"y\"] + pred[\"height\"] / 2\n",
    "            box = [x1, y1, x2, y2]\n",
    "            all_boxes.append(box)\n",
    "            all_scores.append(pred[\"confidence\"])\n",
    "            all_labels.append(pred[\"class\"])\n",
    "\n",
    "    if not all_boxes:\n",
    "        return []\n",
    "\n",
    "    boxes = torch.tensor(all_boxes)\n",
    "    scores = torch.tensor(all_scores)\n",
    "    labels = list(all_labels)\n",
    "    keep = nms(boxes, scores, iou_thresh)\n",
    "\n",
    "    return [{\n",
    "        \"bbox\": boxes[i].tolist(),\n",
    "        \"score\": scores[i].item(),\n",
    "        \"label\": labels[i]\n",
    "    } for i in keep]\n",
    "\n",
    "\n",
    "# --- 4. CONVERT BOX TO COCO FORMAT ---\n",
    "\n",
    "def coco_xyxy_to_xywh(bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return [x1, y1, x2 - x1, y2 - y1]\n",
    "\n",
    "\n",
    "# --- 5. MAIN LOOP ---\n",
    "\n",
    "evaluator = CocoEvaluator(coco_gt=coco_gt, iou_types=[\"bbox\"])\n",
    "\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    image_id = image_ids[idx]\n",
    "\n",
    "    try:\n",
    "        original_img = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {image_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    tta_images = get_augmented_images(original_img)\n",
    "    img_width = original_img.width\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for name, aug_tensor in tta_images:\n",
    "        aug_pil = TF.to_pil_image(aug_tensor)\n",
    "        result = CLIENT.infer(aug_pil, model_id=MODEL_ID)\n",
    "\n",
    "        # Correct predictions if flipped\n",
    "        if name == \"hflip\":\n",
    "            result = undo_horizontal_flip(result, img_width)\n",
    "\n",
    "        all_predictions.append(result)\n",
    "\n",
    "    merged = aggregate_predictions(all_predictions)\n",
    "\n",
    "    # Convert to COCO format\n",
    "    coco_formatted = []\n",
    "    for obj in merged:\n",
    "        label_name = obj[\"label\"]\n",
    "        if label_name not in CLASS_NAME_TO_ID:\n",
    "            continue\n",
    "        coco_formatted.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": CLASS_NAME_TO_ID[label_name],\n",
    "            \"bbox\": coco_xyxy_to_xywh(obj[\"bbox\"]),\n",
    "            \"score\": obj[\"score\"]\n",
    "        })\n",
    "\n",
    "    evaluator.update(coco_formatted)\n",
    "\n",
    "# Final evaluation\n",
    "evaluator.synchronize_between_processes()\n",
    "evaluator.accumulate()\n",
    "evaluator.summarize()\n",
    "\n",
    "# Optional: Print Precision @ IoU 0.5\n",
    "eval_results = evaluator.coco_eval['bbox']\n",
    "precision = eval_results.eval['precision']\n",
    "precision_50 = precision[0, :, -1, 0, 2]\n",
    "valid = precision_50[precision_50 > -1]\n",
    "mean_precision_50 = np.mean(valid)\n",
    "print(f\"Precision @ IoU=0.50: {mean_precision_50:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-gpu-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
