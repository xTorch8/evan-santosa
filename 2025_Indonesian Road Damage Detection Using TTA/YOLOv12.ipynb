{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0492ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from coco_eval import CocoEvaluator\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658b3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy paths - replace these\n",
    "BASE_DIR = \"./Road-Damage-Indonesia-4/test\"\n",
    "ANNOTATION_FILE = \"./Road-Damage-Indonesia-4/test/_annotations.coco.json\"\n",
    "IMAGE_DIR = BASE_DIR\n",
    "\n",
    "# Replace with your Roboflow API info\n",
    "API_KEY = \"pt6GF9eJQFZMOFNFL7sc\"\n",
    "MODEL_ID = \"road-damage-indonesia-fou2m/2\"\n",
    "\n",
    "CLIENT = InferenceHTTPClient(\n",
    "    api_url = \"https://detect.roboflow.com\",\n",
    "    api_key = API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e60d4319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./Road-Damage-Indonesia-4/test\\\\X_27330_jpeg.rf.b1af74b216d0542841dd23ca817fdc71.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_27340_jpeg.rf.4af9e108a84c2761e5fb60b44f0bbdf2.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_9320_jpeg.rf.bae4afd615668c06c2c174cc14d8de6f.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_2810_jpeg.rf.3e8d39d3e615feef9df9ed3f3da06473.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_17270_jpeg.rf.10b9aae5082f769ca285bea4e1985f92.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_31920_jpeg.rf.6de7e47078672dac0d47dc8a8e4e8c48.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\X_6210_jpeg.rf.75e90db30e3482f51d92509585eb2035.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_27220_jpeg.rf.fac59348b40dc5052e7931ef5b9c7272.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_31940_jpeg.rf.d467a1dec0c41c989650f91336e343c7.jpg',\n",
       " './Road-Damage-Indonesia-4/test\\\\Y_29650_jpeg.rf.a3f36abccfaa32ba8b6d870c5c372349.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_gt = COCO(ANNOTATION_FILE)\n",
    "image_ids = coco_gt.getImgIds()\n",
    "image_paths = [os.path.join(IMAGE_DIR, coco_gt.loadImgs(i)[0]['file_name']) for i in image_ids[:10]]\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebebfb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build class name to ID map\n",
    "categories = coco_gt.loadCats(coco_gt.getCatIds())\n",
    "CLASS_NAME_TO_ID = {cat['name']: cat['id'] for cat in categories}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c503b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.129\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.080\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.300\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.094\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.385\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.144\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.231\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.231\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.300\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
      "Precision @ IoU=0.50: 0.351\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image, ImageEnhance\n",
    "from torchvision.ops import nms\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# AUGMENTATION FUNCTIONS\n",
    "# ----------------------------\n",
    "def get_augmented_images(img):\n",
    "    img = img.convert(\"RGB\")\n",
    "    img_tensor = TF.to_tensor(img)\n",
    "    augmentations = []\n",
    "\n",
    "    augmentations.append((\"original\", img_tensor))\n",
    "    augmentations.append((\"hflip\", torch.flip(img_tensor, dims=[2])))\n",
    "\n",
    "    small = TF.resize(img_tensor, [int(img_tensor.shape[1] * 0.75), int(img_tensor.shape[2] * 0.75)])\n",
    "    small = TF.resize(small, [img_tensor.shape[1], img_tensor.shape[2]])\n",
    "    augmentations.append((\"scale_down\", small))\n",
    "\n",
    "    large = TF.resize(img_tensor, [int(img_tensor.shape[1] * 1.25), int(img_tensor.shape[2] * 1.25)])\n",
    "    large = TF.resize(large, [img_tensor.shape[1], img_tensor.shape[2]])\n",
    "    augmentations.append((\"scale_up\", large))\n",
    "\n",
    "    bright = TF.to_tensor(ImageEnhance.Brightness(TF.to_pil_image(img_tensor)).enhance(1.5))\n",
    "    augmentations.append((\"bright\", bright))\n",
    "\n",
    "    contrast = TF.to_tensor(ImageEnhance.Contrast(TF.to_pil_image(img_tensor)).enhance(1.3))\n",
    "    augmentations.append((\"contrast\", contrast))\n",
    "\n",
    "    rotate_p = TF.to_tensor(\n",
    "        TF.rotate(TF.to_pil_image(img_tensor), 15, expand=True).resize((img_tensor.shape[2], img_tensor.shape[1]))\n",
    "    )\n",
    "    augmentations.append((\"rotate_15\", rotate_p))\n",
    "\n",
    "    rotate_m = TF.to_tensor(\n",
    "        TF.rotate(TF.to_pil_image(img_tensor), -15, expand=True).resize((img_tensor.shape[2], img_tensor.shape[1]))\n",
    "    )\n",
    "    augmentations.append((\"rotate_-15\", rotate_m))\n",
    "\n",
    "    return augmentations\n",
    "\n",
    "# ----------------------------\n",
    "# PREDICTION UTILITIES\n",
    "# ----------------------------\n",
    "def undo_horizontal_flip(preds, width):\n",
    "    for pred in preds.get(\"predictions\", []):\n",
    "        pred[\"x\"] = width - pred[\"x\"]\n",
    "    return preds\n",
    "\n",
    "def aggregate_predictions(preds_list, iou_thresh=0.5):\n",
    "    all_boxes, all_scores, all_labels = [], [], []\n",
    "    for preds in preds_list:\n",
    "        for pred in preds.get(\"predictions\", []):\n",
    "            x1 = pred[\"x\"] - pred[\"width\"] / 2\n",
    "            y1 = pred[\"y\"] - pred[\"height\"] / 2\n",
    "            x2 = pred[\"x\"] + pred[\"width\"] / 2\n",
    "            y2 = pred[\"y\"] + pred[\"height\"] / 2\n",
    "            box = [x1, y1, x2, y2]\n",
    "            all_boxes.append(box)\n",
    "            all_scores.append(pred[\"confidence\"])\n",
    "            all_labels.append(pred[\"class\"])\n",
    "\n",
    "    if not all_boxes:\n",
    "        return []\n",
    "\n",
    "    boxes = torch.tensor(all_boxes)\n",
    "    scores = torch.tensor(all_scores)\n",
    "    labels = list(all_labels)\n",
    "    keep = nms(boxes, scores, iou_thresh)\n",
    "\n",
    "    return [{\n",
    "        \"bbox\": boxes[i].tolist(),\n",
    "        \"score\": scores[i].item(),\n",
    "        \"label\": labels[i]\n",
    "    } for i in keep]\n",
    "\n",
    "def coco_xyxy_to_xywh(bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return [x1, y1, x2 - x1, y2 - y1]\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN INFERENCE + EVALUATION\n",
    "# ----------------------------\n",
    "\n",
    "# TODO: Replace these with your actual setup\n",
    "# coco_gt = COCO(\"path_to_groundtruth.json\")\n",
    "# image_paths = [...]  # list of image file paths\n",
    "# image_ids = [...]    # corresponding COCO image IDs\n",
    "# MODEL_ID = \"your-yolov12-model-id\"\n",
    "# CLASS_NAME_TO_ID = {\"cat\": 1, \"dog\": 2, ...}  # your label map\n",
    "# CLIENT = your inference client (must have .infer(image, model_id) -> dict)\n",
    "\n",
    "all_coco_formatted = []\n",
    "\n",
    "for idx, image_path in enumerate(image_paths):\n",
    "    image_id = image_ids[idx]\n",
    "\n",
    "    try:\n",
    "        original_img = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {image_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    tta_images = get_augmented_images(original_img)\n",
    "    img_width = original_img.width\n",
    "    all_predictions = []\n",
    "\n",
    "    for aug_type, aug_tensor in tta_images:\n",
    "        aug_pil = TF.to_pil_image(aug_tensor)\n",
    "        result = CLIENT.infer(aug_pil, model_id=MODEL_ID)\n",
    "\n",
    "        if aug_type == \"hflip\":\n",
    "            result = undo_horizontal_flip(result, img_width)\n",
    "\n",
    "        all_predictions.append(result)\n",
    "\n",
    "    merged = aggregate_predictions(all_predictions)\n",
    "\n",
    "    for obj in merged:\n",
    "        label_name = obj[\"label\"]\n",
    "        if label_name not in CLASS_NAME_TO_ID:\n",
    "            continue\n",
    "        all_coco_formatted.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": CLASS_NAME_TO_ID[label_name],\n",
    "            \"bbox\": coco_xyxy_to_xywh(obj[\"bbox\"]),\n",
    "            \"score\": obj[\"score\"]\n",
    "        })\n",
    "\n",
    "# ----------------------------\n",
    "# COCO Evaluation\n",
    "# ----------------------------\n",
    "if len(all_coco_formatted) > 0:\n",
    "    coco_dt = coco_gt.loadRes(all_coco_formatted)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.imgIds = image_ids[:len(image_paths)]\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    precision = coco_eval.eval['precision']\n",
    "    precision_50 = precision[0, :, -1, 0, 2]\n",
    "    valid = precision_50[precision_50 > -1]\n",
    "    mean_precision_50 = np.mean(valid)\n",
    "    print(f\"Precision @ IoU=0.50: {mean_precision_50:.3f}\")\n",
    "else:\n",
    "    print(\"No valid predictions to evaluate.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-gpu-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
